[//]: # (哈哈我是注释，不会在浏览器中显示。
  Date: 2022-01-16 09:45:34
  LastEditors: gyg
  LastEditTime: 2022-01-19 20:11:53
  FilePath: \test\1_16@磁盘阵列.mm.md
)

# 磁盘阵列

>RAID redundant array of independent disks
独立硬盘冗余阵列
高效的数据组织和条带化的并行访问

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [磁盘阵列](#磁盘阵列)
  - [Raid分类](#raid分类)
    - [特殊控制器（硬件RAID）](#特殊控制器硬件raid)
    - [操作系统驱动程序（软件RAID）](#操作系统驱动程序软件raid)
  - [RAID 的单元](#raid-的单元)
  - [RAID 的数据保护方式](#raid-的数据保护方式)
  - [RAID 常用级别](#raid-常用级别)
    - [raid 0](#raid-0)
    - [raid 1](#raid-1)
    - [raid 3](#raid-3)
    - [raid 5](#raid-5)
    - [Raid 6](#raid-6)
    - [Raid 10](#raid-10)
    - [Raid 50](#raid-50)
  - [RAID 两种备份方法](#raid-两种备份方法)
  - [RAID 状态](#raid-状态)
  - [`mdadm` raid命令](#mdadm-raid命令)
  - [RAID 实验](#raid-实验)
    - [RAID 10 实验](#raid-10-实验)
    - [RAID 10 损坏磁阵列及修复](#raid-10-损坏磁阵列及修复)
    - [RAID 50 磁盘阵列+备份盘](#raid-50-磁盘阵列备份盘)
    - [RAID 50 损坏磁盘](#raid-50-损坏磁盘)

<!-- /code_chunk_output -->

## Raid分类

### 特殊控制器（硬件RAID）

- 优点：

  硬件RAID是专用的处理系统，使用控制器或RAID卡独立于操作系统来管理RAID配置。RAID控制器不会从其管理的磁盘上夺走处理能力。因此，可以使用更多的空间和速度来读取和写入数据。它可以在任何操作系统上运行。更换故障磁盘很简单-只需将其插入并插入新磁盘即可。

- 缺点：

  由于硬件RAID需要额外的控制器硬件，因此成本要高于软件RAID。如果您的RAID控制器发生故障，则必须找到一个兼容的控制器进行更换，以使RAID系统执行设置方式。

### 操作系统驱动程序（软件RAID）

- 优点：

  与硬件RAID不同，软件RAID使用安装RAID磁盘的操作系统的处理能力。成本较低，因为不需要其他硬件RAID控制器。它还允许用户重新配置阵列，而不受硬件RAID控制器的限制。

- 缺点：

  软件RAID往往比硬件RAID慢。由于该软件需要一定的处理能力，因此它会降低RAID配置的读写速度以及在服务器上执行的其他操作。软件RAID通常特定于所使用的操作系统，因此通常不能用于操作系统之间共享的分区。

更换软件RAID中的故障磁盘要复杂一些。您必须首先告诉系统停止使用磁盘，然后更换磁盘。

## RAID 的单元

**条带:** 硬盘中单个或多个连续的扇区，是一块硬盘进行一次数据读写的最小单元
**分条:** 同一个硬盘的阵列中，有多个硬盘驱动器上相同编号的条带

## RAID 的数据保护方式

1. 在另外一块冗余的硬盘上保存副本
2. 奇偶检验算法

## RAID 常用级别

### raid 0

>用在UI数据安全要求不高，但是要求廉价，高利用率的情况下

![984d2d6e042102517ccad923690e1c8a](https://s2.loli.net/2022/01/18/Wad9nPD7xpRKs3B.png)

### raid 1

> 主要通过二次读写，实现镜像磁盘，在数据安全很重要的场所使用

![1547b31209f98c382ea1be78e3f02e04](https://s2.loli.net/2022/01/18/hbM1xiy8uwUWZqm.png)

### raid 3

>比raid 0多了奇偶检验，多一块磁盘，但是数据冗余性好

![b938304ac01d5d0de55cee88f53c7e96](https://s2.loli.net/2022/01/18/CLcsVoAPWtEGUq9.png)

### raid 5

>每个磁盘都能存检验，有几个盘就你能产生几个读写盘，比Raid 3减少一个检验盘

![5acbd5e6ce213846a9bd9d97837bd098](https://s2.loli.net/2022/01/18/WmHZxS2T5QrPOYM.png)

### Raid 6

>`Raid 6 P+Q` ,`RAID6 DP` 两种校验算法，至少需要N+2个磁盘来构成阵列，一般用在数据可靠性极高，可用性极高的场所

![43029fe75b76ae4161b2bdd4e25e0cb5](https://s2.loli.net/2022/01/18/STygJX3o8BahlFQ.png)

### Raid 10

>将镜像Raid和条带进行组合，先进行Raid 1镜像，然后再做Raid 0

![e9041441fac0fed719ed43aa7ab4df12](https://s2.loli.net/2022/01/18/4nGqJS1EwYduZ3h.png)

### Raid 50

>使用了Raid 5 和Raid 0来混合使用，第一级Raid 5,第二级是raid 0

![058019ac9eda7936182540d78b4a4080](https://s2.loli.net/2022/01/18/eYxCtoQZLFRA2lb.png)

## RAID 两种备份方法

1. 热备份
2. 重构

## RAID 状态

1. 创建raid
2. 创建成功 raid 工作
3. 发生故障 Raid降级
4. 等待假设重构成功，则回到2状态
5. 如果失败则raid直接失效

## `mdadm` raid命令

| 参数  |       作用       |
| :---: | :--------------: |
|  -a   |   检测设备名称   |
|  -n   |   指定设备数量   |
|  -l   |   指定RAID级别   |
|  -C   |       创建       |
|  -v   |     显示过程     |
|  -f   |   模拟设备损坏   |
|  -r   |     移除设备     |
|  -Q   |   查看摘要信息   |
|  -D   |   查看详细信息   |
|  -S   | 停止RAID磁盘阵列 |

## RAID 实验

>通过linux服务器

### RAID 10 实验

通过linux服务器添加4块新硬盘，完成Raid10的部署

```bash
[root@client ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0            11:0    1  6.6G  0 rom  /run/media/root/RHEL-8-0-0-BaseOS-x86_64
nvme0n1       259:0    0   50G  0 disk 
├─nvme0n1p1   259:1    0    1G  0 part /boot
└─nvme0n1p2   259:2    0   49G  0 part 
  ├─rhel-root 253:0    0   47G  0 lvm  /
  └─rhel-swap 253:1    0    2G  0 lvm  [SWAP]
nvme0n2       259:3    0   20G  0 disk 
nvme0n3       259:4    0   20G  0 disk 
nvme0n4       259:5    0   20G  0 disk 
nvme0n5       259:6    0   20G  0 disk 

[root@wentan ~]# mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/nvme0n2 /dev/nvme0n3 /dev/nvme0n4 /dev/nvme0n5
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: size set to 20954112K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

#查看四块硬盘Raid等级
[root@wentan ~]# lsblk 
NAME          MAJ:MIN RM  SIZE RO TYPE   MOUNTPOINT
sr0            11:0    1  6.6G  0 rom    /run/media/root/RHEL-8-0-0-BaseOS-x86_64
nvme0n1       259:0    0   25G  0 disk   
├─nvme0n1p1   259:1    0    1G  0 part   /boot
└─nvme0n1p2   259:2    0   24G  0 part   
  ├─rhel-root 253:0    0   22G  0 lvm    /
  └─rhel-swap 253:1    0    2G  0 lvm    [SWAP]
nvme0n2       259:3    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
nvme0n3       259:4    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
nvme0n4       259:5    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
nvme0n5       259:6    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10

#把/dev/md0格式化成ext4文件系统
[root@wentan ~]# mkfs.ext4 /dev/md0 
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 10477056 4k blocks and 2621440 inodes
Filesystem UUID: e1feb5ef-c7f1-4189-a976-d9661edcbfb9
Superblock backups stored on blocks: 
    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
    4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information: done

#创建挂载点，把硬盘设备挂载到挂载点
[root@wentan ~]# mkdir /RAID
[root@wentan ~]# mount /dev/md0 /RAID
[root@wentan ~]# df -h
Filesystem             Size  Used Avail Use% Mounted on
devtmpfs               1.9G     0  1.9G   0% /dev
tmpfs                  1.9G     0  1.9G   0% /dev/shm
tmpfs                  1.9G   10M  1.9G   1% /run
tmpfs                  1.9G     0  1.9G   0% /sys/fs/cgroup
/dev/mapper/rhel-root   22G  4.2G   18G  19% /
/dev/nvme0n1p1        1014M  169M  846M  17% /boot
tmpfs                  376M   16K  376M   1% /run/user/42
tmpfs                  376M  2.3M  374M   1% /run/user/0
/dev/sr0               6.7G  6.7G     0 100% /run/media/root/RHEL-8-0-0-BaseOS-x86_64
/dev/md0                40G   49M   38G   1% /RAID

#通过—D命令查看详细信息
[root@wentan RAID]# mdadm -D /dev/md0 
/dev/md0:
           Version : 1.2
     Creation Time : Sun Jan 16 11:41:10 2022
        Raid Level : raid10
        Array Size : 41908224 (39.97 GiB 42.91 GB)
     Used Dev Size : 20954112 (19.98 GiB 21.46 GB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Sun Jan 16 11:47:08 2022
             State : clean 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

              Name : wentan:0  (local to host wentan)
              UUID : 096170c1:5b9e7606:f4e8c353:cd0c020e
            Events : 19

    Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync set-A   /dev/nvme0n2
       1     259        4        1      active sync set-B   /dev/nvme0n3
       2     259        5        2      active sync set-A   /dev/nvme0n4
       3     259        6        3      active sync set-B   /dev/nvme0n5
```

当确认无误后，我们需要对系统做永久挂载，永久挂载配置文件 /etc/fstab

```bash
 [root@wentan RAID]# vim /etc/fstab
/dev/md0        /RAID   ext4 defaults 0 0
#最后一行加入以上信息，完成后务必mount -a 保存配置文件没有问题，如果报错 必须回去修改配置文件
[root@wentan RAID]# mount -a
```

### RAID 10 损坏磁阵列及修复

>在确认有一块物理硬盘设备出现损坏而不能继续正常使用后，应该使用mdadm命令将其移除，然后查看RAID磁盘阵列的状态，可以发现状态已经改变。

```bash
 #首先我们手动损坏一块硬盘
[root@wentan RAID]# mdadm /dev/md0 -f /dev/nvme0n3
mdadm: set /dev/nvme0n3 faulty in /dev/md0
[root@wentan RAID]# mdadm -D /dev/md0
    Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync set-A   /dev/nvme0n2
       -       0        0        1      removed
       2     259        5        2      active sync set-A   /dev/nvme0n4
       3     259        6        3      active sync set-B   /dev/nvme0n5

       1     259        4        -      faulty   /dev/nvme0n3
```

在Raid 10级别的磁盘阵列中，当RAID 1磁盘阵列中存在一个故障盘时并不影响RAID 10磁盘阵列的使用。当购买了新的硬盘设备后再使用mdadm命令来予以替换即可，在此期间我们可以在/RAID目录中正常地创建或删除文件。由于我们是在虚拟机中模拟硬盘，所以先重启系统，然后再把新的硬盘添加到RAID磁盘阵列中。

```bash
 #取消挂载
[root@wentan ~]# umount /RAID
#添加新的磁盘
[root@wentan ~]# mdadm /dev/md0 -a /dev/nvme0n3
mdadm: added /dev/nvme0n3
Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync set-A   /dev/nvme0n2
       4     259        4        1      spare rebuilding   /dev/nvme0n3
       2     259        5        2      active sync set-A   /dev/nvme0n4
       3     259        6        3      active sync set-B   /dev/nvme0n5
#此时发现新添加的硬盘正在重构，最后可以 mount -a 把刚刚的挂载挂上去
[root@wentan ~]# mount -a
```

### RAID 50 磁盘阵列+备份盘

>为了避免多个实验之间相互发生冲突，我们需要保证每个实验的相对独立性，为此需要大家自行将虚拟机还原到初始状态。

部署RAID 5磁盘阵列时，至少需要用到3块硬盘，还需要再加一块备份硬盘，所以总计需要在虚拟机中模拟4块硬盘设备

现在创建一个RAID 5磁盘阵列+备份盘。在下面的命令中，参数-n 3代表创建这个RAID 5磁盘阵列所需的硬盘数，参数-l 5代表RAID的级别，而参数-x 1则代表有一块备份盘。当查看/dev/md0（即RAID 5磁盘阵列的名称）磁盘阵列的时候就能看到有一块备份盘在等待中了。

```bash
[root@wentan ~] # mdadm -Cv /dev/md0 -n 3 -l 5 -x 1 /dev/nvme0n2 /dev/nvme0n3 /dev/nvme0n4 /dev/nvme0n5
mdadm: layout defaults to left-symmetric
mdadm: layout defaults to left-symmetric
mdadm: chunk size defaults to 512K
mdadm: size set to 20954112K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

#查看磁盘状态，有一块备份盘
[root@wentan ~]# mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Sun Jan 16 13:50:08 2022
        Raid Level : raid5
        Array Size : 41908224 (39.97 GiB 42.91 GB)
     Used Dev Size : 20954112 (19.98 GiB 21.46 GB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Sun Jan 16 13:51:57 2022
             State : clean 
    Active Devices : 3
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 1

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : wentan:0  (local to host wentan)
              UUID : 0d7fd374:a2001c3d:737de6c9:d3baae39
            Events : 18

    Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync   /dev/nvme0n2
       1     259        4        1      active sync   /dev/nvme0n3
       4     259        5        2      active sync   /dev/nvme0n4

       3     259        6        -      spare   /dev/nvme0n5

#把/dev/md0格式化成ext4文件系统     
[root@wentan ~]# mkfs.ext4 /dev/md0 
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 10477056 4k blocks and 2621440 inodes
Filesystem UUID: 68f42614-5bd2-40d5-88ee-99cd714d3ad0
Superblock backups stored on blocks: 
    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
    4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information: done   

#创建挂载点，把硬盘设备挂载到挂载点
[root@wentan ~]# mkdir /RAID
[root@wentan ~]# echo "/dev/md0 /RAID ext4 defaults 0 0" >> /etc/fstab
#挂载
[root@wentan ~]# mount -a  

[root@wentan ~]# df -h
Filesystem             Size  Used Avail Use% Mounted on
/dev/md0                40G   49M   38G   1% /RAID
```

### RAID 50 损坏磁盘

```bash
 #手动损坏一块硬盘
[root@wentan ~]# mdadm /dev/md0 -f /dev/nvme0n3
mdadm: set /dev/nvme0n3 faulty in /dev/md0
[root@wentan ~]# mdadm -D /dev/md0
Number   Major   Minor   RaidDevice State
0     259        3        0      active sync   /dev/nvme0n2
3     259        6        1      spare rebuilding   /dev/nvme0n5
4     259        5        2      active sync   /dev/nvme0n4

1     259        4        -      faulty   /dev/nvme0n3
#备份硬盘自动顶上  开始重构。
```
