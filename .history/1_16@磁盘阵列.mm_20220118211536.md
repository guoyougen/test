[//]: # (哈哈我是注释，不会在浏览器中显示。
  Date: 2022-01-16 09:45:34
  LastEditors: gyg
  LastEditTime: 2022-01-18 21:15:36
  FilePath: \test\1_16@磁盘阵列.mm.md
)

# 磁盘阵列

>RAID redundant array of independent disks
独立硬盘冗余阵列
高效的数据组织和条带化的并行访问

## raid分类

### 特殊控制器（硬件RAID）

- 优点：

  硬件RAID是专用的处理系统，使用控制器或RAID卡独立于操作系统来管理RAID配置。RAID控制器不会从其管理的磁盘上夺走处理能力。因此，可以使用更多的空间和速度来读取和写入数据。它可以在任何操作系统上运行。更换故障磁盘很简单-只需将其插入并插入新磁盘即可。

- 缺点：

  由于硬件RAID需要额外的控制器硬件，因此成本要高于软件RAID。如果您的RAID控制器发生故障，则必须找到一个兼容的控制器进行更换，以使RAID系统执行设置方式。

### 操作系统驱动程序（软件RAID）

- 优点：

  与硬件RAID不同，软件RAID使用安装RAID磁盘的操作系统的处理能力。成本较低，因为不需要其他硬件RAID控制器。它还允许用户重新配置阵列，而不受硬件RAID控制器的限制。

- 缺点：

  软件RAID往往比硬件RAID慢。由于该软件需要一定的处理能力，因此它会降低RAID配置的读写速度以及在服务器上执行的其他操作。软件RAID通常特定于所使用的操作系统，因此通常不能用于操作系统之间共享的分区。

更换软件RAID中的故障磁盘要复杂一些。您必须首先告诉系统停止使用磁盘，然后更换磁盘。

## RAID 的单元

**条带:** 硬盘中单个或多个连续的扇区，是一块硬盘进行一次数据读写的最小单元
**分条:** 同一个硬盘的阵列中，有多个硬盘驱动器上相同编号的条带

## RAID 的数据保护方式

1. 在另外一块冗余的硬盘上保存副本
2. 奇偶检验算法

## RAID 常用级别

### raid 0

>用在UI数据安全要求不高，但是要求廉价，高利用率的情况下

![984d2d6e042102517ccad923690e1c8a](https://s2.loli.net/2022/01/18/Wad9nPD7xpRKs3B.png)

### raid 1

> 主要通过二次读写，实现镜像磁盘，在数据安全很重要的场所使用

![1547b31209f98c382ea1be78e3f02e04](https://s2.loli.net/2022/01/18/hbM1xiy8uwUWZqm.png)

### raid 3

>比raid 0多了奇偶检验，多一块磁盘，但是数据冗余性好

![b938304ac01d5d0de55cee88f53c7e96](https://s2.loli.net/2022/01/18/CLcsVoAPWtEGUq9.png)

### raid 5

>每个磁盘都能存检验，有几个盘就你能产生几个读写盘，比Raid 3减少一个检验盘

![5acbd5e6ce213846a9bd9d97837bd098](https://s2.loli.net/2022/01/18/WmHZxS2T5QrPOYM.png)

### Raid 6

>`Raid 6 P+Q` ,`RAID6 DP` 两种校验算法，至少需要N+2个磁盘来构成阵列，一般用在数据可靠性极高，可用性极高的场所

![43029fe75b76ae4161b2bdd4e25e0cb5](https://s2.loli.net/2022/01/18/STygJX3o8BahlFQ.png)

### Raid 10

>将镜像Raid和条带进行组合，先进行Raid 1镜像，然后再做Raid 0

![e9041441fac0fed719ed43aa7ab4df12](https://s2.loli.net/2022/01/18/4nGqJS1EwYduZ3h.png)

### Raid 50

>使用了Raid 5 和Raid 0来混合使用，第一级Raid 5,第二级是raid 0

![058019ac9eda7936182540d78b4a4080](https://s2.loli.net/2022/01/18/eYxCtoQZLFRA2lb.png)

## RAID 两种备份方法

1. 热备份
2. 重构

## RAID 状态

1. 创建raid
2. 创建成功 raid 工作
3. 发生故障 Raid降级
4. 等待假设重构成功，则回到2状态
5. 如果失败则raid直接失效

## RAID 实验

>通过linux服务器 

### RAID 10 实验

```bash
[root@client ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0            11:0    1  6.6G  0 rom  /run/media/root/RHEL-8-0-0-BaseOS-x86_64
nvme0n1       259:0    0   50G  0 disk 
├─nvme0n1p1   259:1    0    1G  0 part /boot
└─nvme0n1p2   259:2    0   49G  0 part 
  ├─rhel-root 253:0    0   47G  0 lvm  /
  └─rhel-swap 253:1    0    2G  0 lvm  [SWAP]
nvme0n2       259:3    0   20G  0 disk 
nvme0n3       259:4    0   20G  0 disk 
nvme0n4       259:5    0   20G  0 disk 
nvme0n5       259:6    0   20G  0 disk 
```

mdadm 命令



```bash
[root@client ~]# mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/nvme0n2 /dev/nvme0n3 /dev/nvme0n4 /dev/nvme0n5
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: size set to 20954112K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
```

```bash
[root@client ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0            11:0    1  6.6G  0 rom  /run/media/root/RHEL-8-0-0-BaseOS-x86_64
nvme0n1       259:0    0   50G  0 disk 
├─nvme0n1p1   259:1    0    1G  0 part /boot
└─nvme0n1p2   259:2    0   49G  0 part 
  ├─rhel-root 253:0    0   47G  0 lvm  /
  └─rhel-swap 253:1    0    2G  0 lvm  [SWAP]
nvme0n2       259:3    0   20G  0 disk 
nvme0n3       259:4    0   20G  0 disk 
nvme0n4       259:5    0   20G  0 disk 
nvme0n5       259:6    0   20G  0 disk 
[root@client ~]# ^C
[root@client ~]# mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/nvme0n2 /dev/nvme0n3 /dev/ncme0n4 /dev/nvme0n5
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: cannot open /dev/ncme0n4: No such file or directory
[root@client ~]# mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/nvme0n2 /dev/nvme0n3 /dev/nvme0n4 /dev/nvme0n5
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: size set to 20954112K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
[root@client ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE   MOUNTPOINT
sr0            11:0    1  6.6G  0 rom    /run/media/root/RHEL-8-0-0-BaseOS-x86_64
nvme0n1       259:0    0   50G  0 disk   
├─nvme0n1p1   259:1    0    1G  0 part   /boot
└─nvme0n1p2   259:2    0   49G  0 part   
  ├─rhel-root 253:0    0   47G  0 lvm    /
  └─rhel-swap 253:1    0    2G  0 lvm    [SWAP]
nvme0n2       259:3    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
nvme0n3       259:4    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
nvme0n4       259:5    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
nvme0n5       259:6    0   20G  0 disk   
└─md0           9:0    0   40G  0 raid10 
[root@client ~]# mkfs.
mkfs.cramfs  mkfs.ext3    mkfs.fat     mkfs.msdos   mkfs.xfs     
mkfs.ext2    mkfs.ext4    mkfs.minix   mkfs.vfat    
[root@client ~]# mkfs.ext4 /dev/md0
mke2fs 1.44.3 (10-July-2018)
Creating filesystem with 10477056 4k blocks and 2621440 inodes
Filesystem UUID: a911d5ac-fe52-4937-8876-068ca461b189
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000, 7962624

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (65536 blocks): done
Writing superblocks and filesystem accounting information: done   
```

```bash
[root@client ~]# mkdir /RAID
[root@client ~]# mount /dev/md0  /RAID
[root@client ~]# df -h
Filesystem             Size  Used Avail Use% Mounted on
devtmpfs               1.9G     0  1.9G   0% /dev
tmpfs                  1.9G     0  1.9G   0% /dev/shm
tmpfs                  1.9G   18M  1.9G   1% /run
tmpfs                  1.9G     0  1.9G   0% /sys/fs/cgroup
/dev/mapper/rhel-root   47G  4.3G   43G   9% /
/dev/nvme0n1p1        1014M  169M  846M  17% /boot
tmpfs                  376M   16K  376M   1% /run/user/42
tmpfs                  376M  2.3M  374M   1% /run/user/0
/dev/sr0               6.7G  6.7G     0 100% /run/media/root/RHEL-8-0-0-BaseOS-x86_64
/dev/md0                40G   49M   38G   1% /RAID
```
```bash
[root@client /]# mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Sun Jan 16 08:41:38 2022
        Raid Level : raid10
        Array Size : 41908224 (39.97 GiB 42.91 GB)
     Used Dev Size : 20954112 (19.98 GiB 21.46 GB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Sun Jan 16 08:47:04 2022
             State : clean 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

              Name : client:0  (local to host client)
              UUID : 1d71a341:0def5da9:e267830b:1a8d3b58
            Events : 19

    Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync set-A   /dev/nvme0n2
       1     259        4        1      active sync set-B   /dev/nvme0n3
       2     259        5        2      active sync set-A   /dev/nvme0n4
       3     259        6        3      active sync set-B   /dev/nvme0n5
```

```bash
[root@client /]# vim /etc/fstab 
```

```bash
/dev/mapper/rhel-root   /                       xfs     defaults        0 0
UUID=0e9a3bec-a90b-438c-8ce9-43a9c492fb69 /boot                   xfs     defaults        0 0
/dev/mapper/rhel-swap   swap                    swap    defaults        0 0
/dev/md0                /RAID                   ext4    defaults        0 0     
~                                                                              
```

```bash
[root@client /]# mount -a
```

### 故障

```bash
[root@client ~]# mdadm /dev/md0 -f /dev/nvme0n3
mdadm: set /dev/nvme0n3 faulty in /dev/md0
[root@client ~]# mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Sun Jan 16 08:41:38 2022
        Raid Level : raid10
        Array Size : 41908224 (39.97 GiB 42.91 GB)
     Used Dev Size : 20954112 (19.98 GiB 21.46 GB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Sun Jan 16 10:33:35 2022
             State : clean, degraded 
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 1
     Spare Devices : 0

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

              Name : client:0  (local to host client)
              UUID : 1d71a341:0def5da9:e267830b:1a8d3b58
            Events : 21

    Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync set-A   /dev/nvme0n2
       -       0        0        1      removed
       2     259        5        2      active sync set-A   /dev/nvme0n4
       3     259        6        3      active sync set-B   /dev/nvme0n5

       1     259        4        -      faulty   /dev/nvme0n3
```
### 备份

```bash
[root@localhost ~]# mdadm -Cv /dev/md0 -n 3 -l 5 -x 1 /dev/nvme0n2 /dev/nvme0n3 /dev/nvme0n4 /dev/nvme0n5
mdadm: layout defaults to left-symmetric
mdadm: layout defaults to left-symmetric
mdadm: chunk size defaults to 512K
mdadm: size set to 20954112K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
[root@localhost ~]# 
```

```bash
[root@localhost ~]# mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Sun Jan 16 10:50:21 2022
        Raid Level : raid5
        Array Size : 41908224 (39.97 GiB 42.91 GB)
     Used Dev Size : 20954112 (19.98 GiB 21.46 GB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Sun Jan 16 10:52:07 2022
             State : clean 
    Active Devices : 3
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 1

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : localhost.localdomain:0  (local to host localhost.localdomain)
              UUID : 1c084e54:7d8b710e:ddd0f558:605cfc6e
            Events : 18

    Number   Major   Minor   RaidDevice State
       0     259        3        0      active sync   /dev/nvme0n2
       1     259        4        1      active sync   /dev/nvme0n3
       4     259        5        2      active sync   /dev/nvme0n4

       3     259        6        -      spare   /dev/nvme0n5
```